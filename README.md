# ğŸ” LangGraph RAG Workflow â€” Document Retrieval & Answer Validation

Este projeto implementa um pipeline completo de RAG (Retrieval-Augmented Generation) utilizando a biblioteca [LangGraph](https://python.langchain.com/docs/langgraph), com validaÃ§Ãµes automÃ¡ticas usando LLMs para garantir que os documentos recuperados sÃ£o relevantes e que as respostas geradas sÃ£o Ãºteis e baseadas no contexto.

## ğŸ“Œ Objetivo

Criar um sistema de perguntas e respostas robusto que:

1. **Recupere documentos** de uma base vetorizada (Vectorstore)
2. **Filtre documentos irrelevantes** com um LLM-grader
3. **Gere respostas** baseadas no contexto dos documentos
4. **Valide as respostas** quanto Ã  confiabilidade (sem alucinaÃ§Ã£o) e utilidade

---

## ğŸ“ Estrutura do Pipeline

O fluxo Ã© implementado como um grafo com os seguintes nÃ³s:

UsuÃ¡rio âœ route_question
â”œâ”€â”€ websearch â”€â”
â”‚ â†“
â””â”€â”€ retrieve âœ grade_documents â”€â”€â”
â”‚ â†“
â””â”€> websearch â†’ generate â†’ grade_generation_v_documents_and_question
â”œâ”€â”€ useful â†’ END
â”œâ”€â”€ not useful â†’ websearch
â””â”€â”€ not supported â†’ generate

yaml
Copiar
Editar

---

## âš™ï¸ Tecnologias e Bibliotecas

- [LangChain](https://www.langchain.com/)
- [LangGraph](https://python.langchain.com/docs/langgraph)
- [OpenAI GPT Models](https://platform.openai.com/)
- Tipos do Python: `TypedDict`, `List`, etc.

---

## ğŸ“¦ Requisitos

- Python 3.10+
- API Key da OpenAI
- Vectorstore configurado (Ex: FAISS, Pinecone, Chroma, etc.)
- Um modelo LLM para:
  - Gerar resposta (`rag_chain`)
  - Avaliar relevÃ¢ncia (`retrieval_grader`)
  - Detectar alucinaÃ§Ã£o (`hallucination_grader`)
  - Avaliar se a resposta responde Ã  pergunta (`answer_grader`)

---

## ğŸ§ª Como rodar

1. Instale os pacotes necessÃ¡rios:
```bash
pip install langchain langgraph openai tiktoken
Configure suas variÃ¡veis de ambiente:

bash
Copiar
Editar
export OPENAI_API_KEY="sua-chave-aqui"
Invoque o workflow:

python
Copiar
Editar
from langgraph.graph import END

# Inicialize o app
app = workflow.compile()

# Estado inicial
state = {
    "question": "How do agents use memory in LangChain?",
    "documents": [],
    "web_search": "No",
    "generation": ""
}

# Execute o grafo
result = app.invoke(state)
print(result)
ğŸ§  Componentes Chave
retrieve(state)
Recupera documentos do vectorstore com base na pergunta.

grade_documents(state)
Filtra os documentos que nÃ£o sÃ£o relevantes usando um LLM com prompt de validaÃ§Ã£o.

generate(state)
Gera uma resposta com base nos documentos relevantes (RAG).

grade_generation_v_documents_and_question(state)
Valida se a resposta estÃ¡ fundamentada nos documentos e se responde adequadamente Ã  pergunta.

web_search(state)
Busca resultados adicionais na web, caso o vectorstore falhe.

route_question(state)
Decide se a busca deve comeÃ§ar pela web ou pelo vectorstore.

âœ… Exemplo de uso do retrieval_grader
python
Copiar
Editar
result = retrieval_grader.invoke({
  "question": "agent memory",
  "document": "LangChain agents can store short-term context using memory tools."
})
print(result)  # {"score": "yes"}
ğŸ§© ExtensÃµes possÃ­veis
IntegraÃ§Ã£o com Pinecone ou ChromaDB como vectorstore

Uso de agentes com memÃ³ria para manter o contexto entre rodadas

Logging e mÃ©tricas de qualidade das respostas
